diff -urpN linux/include/linux/sched/prio.h bmq-b/include/linux/sched/prio.h
--- linux/include/linux/sched/prio.h	2024-01-29 14:39:28.204327321 +0800
+++ bmq-b/include/linux/sched/prio.h	2024-01-29 14:45:58.575295995 +0800
@@ -26,7 +26,7 @@
 
 /* +/- priority levels from the base priority */
 #ifdef CONFIG_SCHED_BMQ
-#define MAX_PRIORITY_ADJ	(7)
+#define MAX_PRIORITY_ADJ	(12)
 
 #define MIN_NORMAL_PRIO		(MAX_RT_PRIO)
 #define MAX_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH)
diff -urpN linux/kernel/sched/alt_core.c bmq-b/kernel/sched/alt_core.c
--- linux/kernel/sched/alt_core.c	2024-01-29 14:39:28.317172465 +0800
+++ bmq-b/kernel/sched/alt_core.c	2024-01-29 14:50:10.234372760 +0800
@@ -608,7 +608,8 @@ static inline void update_rq_clock(struc
 	if (unlikely(delta <= 0))
 		return;
 	rq->clock += delta;
-	update_rq_time_edge(rq);
+	// update_rq_time_edge(rq);
+	sched_update_rq_clock(rq);
 	update_rq_clock_task(rq, delta);
 }
 
@@ -2989,6 +2990,7 @@ int try_to_wake_up(struct task_struct *p
 			set_task_cpu(p, cpu);
 		}
 #else
+		sched_task_ttwu(p);
 		cpu = task_cpu(p);
 #endif /* CONFIG_SMP */
 
@@ -4144,7 +4146,7 @@ void scheduler_tick(void)
 
 	task_tick_mm_cid(rq, rq->curr);
 
-	rq->last_tick = rq->clock;
+	// rq->last_tick = rq->clock;
 	raw_spin_unlock(&rq->lock);
 
 	if (sched_feat(LATENCY_WARN) && resched_latency)
@@ -4632,6 +4634,16 @@ static inline int take_other_rq_tasks(st
 }
 #endif
 
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	p->time_slice = sched_timeslice_ns;
+
+	sched_task_renew(p, rq);
+
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
+		requeue_task(p, rq, task_sched_prio_idx(p, rq));
+}
+
 /*
  * Timeslices below RESCHED_NS are considered as good as expired as there's no
  * point rescheduling when there's so little time left.
@@ -4840,9 +4852,11 @@ static void __sched notrace __schedule(u
 #endif
 
 	if (likely(prev != next)) {
-		next->last_ran = rq->clock_task;
+		//next->last_ran = rq->clock_task;
+#ifdef CONFIG_SCHED_BMQ
 		rq->last_ts_switch = rq->clock;
-
+#endif
+		next->last_ran = rq->clock_task;
 		/*printk(KERN_INFO "sched: %px -> %px\n", prev, next);*/
 		rq->nr_switches++;
 		/*
diff -urpN linux/kernel/sched/alt_sched.h bmq-b/kernel/sched/alt_sched.h
--- linux/kernel/sched/alt_sched.h	2024-01-29 14:39:28.317172465 +0800
+++ bmq-b/kernel/sched/alt_sched.h	2024-01-29 14:53:09.868235699 +0800
@@ -14,16 +14,27 @@
 
 #include "cpupri.h"
 
-#ifdef CONFIG_SCHED_BMQ
+//#ifdef CONFIG_SCHED_BMQ
 /* bits:
  * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_LEVELS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
-#endif
+//#define SCHED_LEVELS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
+//#endif
 
-#ifdef CONFIG_SCHED_PDS
+//#ifdef CONFIG_SCHED_PDS
 /* bits: RT(0-24), reserved(25-31), SCHED_NORMAL_PRIO_NUM(32), cpu idle task(1) */
-#define SCHED_LEVELS	(64 + 1)
-#endif /* CONFIG_SCHED_PDS */
+//#define SCHED_LEVELS	(64 + 1)
+//#endif /* CONFIG_SCHED_PDS */
+
+#define MIN_SCHED_NORMAL_PRIO	(32)
+/*
+ * levels: RT(0-24), reserved(25-31), NORMAL(32-63), cpu idle task(64)
+ *
+ * -- BMQ --
+ * NORMAL: (lower boost range 12, NICE_WIDTH 40, higher boost range 12) / 2
+ * -- PDS --
+ * NORMAL: SCHED_EDGE_DELTA + ((NICE_WIDTH 40) / 2)
+ */
+#define SCHED_LEVELS		(64 + 1)
 
 #define IDLE_TASK_SCHED_PRIO	(SCHED_LEVELS - 1)
 
@@ -206,7 +217,9 @@ struct rq {
 	u64			clock ____cacheline_aligned;
 	u64			last_tick;
 	u64			clock_task;
+#ifdef CONFIG_SCHED_BMQ
 	u64			last_ts_switch;
+#endif
 
 	unsigned int  nr_running;
 	unsigned long nr_uninterruptible;
diff -urpN linux/kernel/sched/bmq.h bmq-b/kernel/sched/bmq.h
--- linux/kernel/sched/bmq.h	2024-01-29 14:39:28.317172465 +0800
+++ bmq-b/kernel/sched/bmq.h	2024-01-29 14:55:52.970285048 +0800
@@ -4,8 +4,9 @@
  * BMQ only routines
  */
 #define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
-#define boost_threshold(p)	(sched_timeslice_ns >>\
-				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
+//#define boost_threshold(p)	(sched_timeslice_ns >>\
+//				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
+#define boost_threshold(p)	(sched_timeslice_ns >> ((14 - (p)->boost_prio) / 2))
 
 static inline void boost_task(struct task_struct *p)
 {
@@ -46,7 +47,9 @@ task_sched_prio_normal(const struct task
 
 static inline int task_sched_prio(const struct task_struct *p)
 {
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+//	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+	return (p->prio < MAX_RT_PRIO)? (p->prio >> 2) :
+		MIN_SCHED_NORMAL_PRIO + (p->prio + p->boost_prio - MAX_RT_PRIO) / 2;
 }
 
 static inline int
@@ -65,6 +68,7 @@ static inline int sched_idx2prio(int idx
 	return idx;
 }
 
+/*
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
@@ -77,12 +81,17 @@ static inline void time_slice_expired(st
 }
 
 static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
+*/
 
 inline int task_running_nice(struct task_struct *p)
 {
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
 }
 
+static inline void sched_update_rq_clock(struct rq *rq) {}
+static inline void sched_task_renew(struct task_struct *p, const struct rq *rq) {}
+static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
+
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = MAX_PRIORITY_ADJ;
@@ -93,18 +102,23 @@ static inline void do_sched_yield_type_1
 	p->boost_prio = MAX_PRIORITY_ADJ;
 }
 
-#ifdef CONFIG_SMP
+//#ifdef CONFIG_SMP
 static inline void sched_task_ttwu(struct task_struct *p)
 {
 	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
 		boost_task(p);
 }
-#endif
+//#endif
 
 static inline void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 {
-	if (rq_switch_time(rq) < boost_threshold(p))
+	//if (rq_switch_time(rq) < boost_threshold(p))
+	u64 switch_ns = rq_switch_time(rq);
+
+	if (switch_ns < boost_threshold(p))
 		boost_task(p);
+	else if (switch_ns > sched_timeslice_ns)
+		deboost_task(p);
 }
 
-static inline void update_rq_time_edge(struct rq *rq) {}
+// static inline void update_rq_time_edge(struct rq *rq) {}
diff -urpN linux/kernel/sched/pds.h bmq-b/kernel/sched/pds.h
--- linux/kernel/sched/pds.h	2024-01-29 14:39:28.317172465 +0800
+++ bmq-b/kernel/sched/pds.h	2024-01-29 14:57:37.587337300 +0800
@@ -1,6 +1,6 @@
 #define ALT_SCHED_NAME "PDS"
 
-#define MIN_SCHED_NORMAL_PRIO	(32)
+//#define MIN_SCHED_NORMAL_PRIO	(32)
 static const u64 RT_MASK = ((1ULL << MIN_SCHED_NORMAL_PRIO) - 1);
 
 #define SCHED_NORMAL_PRIO_NUM	(32)
@@ -68,18 +68,21 @@ static inline int sched_idx2prio(int sch
 		MIN_SCHED_NORMAL_PRIO + SCHED_NORMAL_PRIO_MOD(sched_idx - rq->time_edge);
 }
 
+/*
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
 {
 	if (p->prio >= MIN_NORMAL_PRIO)
 		p->deadline = rq->time_edge + (p->static_prio - (MAX_PRIO - NICE_WIDTH)) / 2;
 }
+*/
 
 int task_running_nice(struct task_struct *p)
 {
 	return (p->prio > DEFAULT_PRIO);
 }
 
-static inline void update_rq_time_edge(struct rq *rq)
+//static inline void update_rq_time_edge(struct rq *rq)
+static inline void sched_update_rq_clock(struct rq *rq)
 {
 	struct list_head head;
 	u64 old = rq->time_edge;
@@ -121,7 +124,8 @@ static inline void update_rq_time_edge(s
 		MIN_SCHED_NORMAL_PRIO : rq->prio - delta;
 }
 
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+//static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+static inline void sched_task_renew(struct task_struct *p, const struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
 	sched_renew_deadline(p, rq);
@@ -138,7 +142,8 @@ static inline void sched_task_sanity_che
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
-	sched_renew_deadline(p, rq);
+//	sched_renew_deadline(p, rq);
+	sched_task_renew(p, rq);
 }
 
 static inline void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
@@ -146,7 +151,7 @@ static inline void do_sched_yield_type_1
 	time_slice_expired(p, rq);
 }
 
-#ifdef CONFIG_SMP
+//#ifdef CONFIG_SMP
 static inline void sched_task_ttwu(struct task_struct *p) {}
-#endif
+//#endif
 static inline void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
